{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An end-to-end Vertex HPO Pipeline\n",
    "Some highlights for this work VS the vanilla HPO solutions\n",
    "\n",
    "1.https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb\n",
    "\n",
    "2.https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/hyperparameter_tuning/distributed-hyperparameter-tuning.ipynb\n",
    "\n",
    "1. Supports pipeline input data. We added a worker pool spec generator to receive the output of preprocess and consume as an argument of the HPO job.\n",
    "2. Supports multiple sub groups at the same time i.e. multiple warehouses.\n",
    "3. Supports HPO params logged in firestore. \n",
    "4. Supports training kickoff after HPO.\n",
    "\n",
    "Validated via the sentiment pipeline.\n",
    "Please be noted that some params flowing in the pipeline are not practically consumed in each step, instead, we aim to validate the chain of individual components.\n",
    "One example is 'warehouse', it's not used to slice any data as it's supposed to do as we are using sentiment analysis data as the example. However, we do validate (print/log) if that param is correctly passed to the list of arguments of relevant steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abb7a68-ed8a-40a4-b49d-10449ee45380",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==1.8.11\n",
      "  Downloading kfp-1.8.11.tar.gz (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.11) (0.15.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.6/636.6 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.11) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.11) (2.1.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.11) (8.1.3)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.13\n",
      "  Downloading kfp_pipeline_spec-0.1.17-py3-none-any.whl (12 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.11) (3.20.3)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.11) (1.10.2)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp==1.8.11) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp==1.8.11) (4.11.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.11) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.11) (1.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.11) (0.20.4)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.11) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.11) (2.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (0.2.7)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (4.2.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.28.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.3.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.11) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.11) (0.18.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.11) (1.26.11)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.11) (2022.9.24)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.11) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.11) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.11) (1.4.1)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.11) (0.37.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.8.11) (1.56.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.11) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.11) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp==1.8.11) (3.10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.11) (3.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.21)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.11-py3-none-any.whl size=414450 sha256=7577ed6ed5d349c4b9b2ef725fca2f5569fb6e7b8c7f98fb3f70cc788d0d3717\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/d6/34/67/6fa40b4b0ed493ece1da9b1d50ff2c8bd9402e7f307fbc9866\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116951 sha256=f04c841357ddedbb01ffa0c1d59fc46fd3e43ff03d90bad205daf471ad8fd87e\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/20/e8/7b/003fc14f02f262dd4614aec55e41147c8012e3dad98c936b76\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99715 sha256=0af63eac9b77c556fefd3a8a79b435ba2ccdc7cdb92c8adbf7f837070ad17755\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/64/f0/9d/dcf844b38d247b944657d0b9e8d55b76baef9bfdeb680e4596\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=ad81127f51249d53129730b3279b127256e4154f043e9231eb7419ae9c64f0e2\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5a/4e/89/a4493443c6aadb8a38e9709610efc3bfafaea3a4108df4c112\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, uritemplate, tabulate, strip-hints, PyYAML, pydantic, kfp-pipeline-spec, fire, docstring-parser, Deprecated, requests-toolbelt, kfp-server-api, jsonschema, typer, kubernetes, google-api-python-client, google-cloud-storage, kfp\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: uritemplate\n",
      "    Found existing installation: uritemplate 4.1.1\n",
      "    Uninstalling uritemplate-4.1.1:\n",
      "      Successfully uninstalled uritemplate-4.1.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.16.0\n",
      "    Uninstalling jsonschema-4.16.0:\n",
      "      Successfully uninstalled jsonschema-4.16.0\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.65.0\n",
      "    Uninstalling google-api-python-client-2.65.0:\n",
      "      Successfully uninstalled google-api-python-client-2.65.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.5.0\n",
      "    Uninstalling google-cloud-storage-2.5.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\n",
      "tfx-bsl 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tfx-bsl 1.10.1 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.5 which is incompatible.\n",
      "tensorflow 2.6.5 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\n",
      "tensorflow 2.6.5 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-transform 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-transform 1.10.1 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.5 which is incompatible.\n",
      "tensorflow-serving-api 2.10.0 requires tensorflow<3,>=2.10.0, but you have tensorflow 2.6.5 which is incompatible.\n",
      "gluonts 0.11.1 requires typing-extensions~=4.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\n",
      "apache-beam 2.42.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n",
      "apache-beam 2.42.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 docstring-parser-0.15 fire-0.5.0 google-api-python-client-1.12.11 google-cloud-storage-1.44.0 jsonschema-3.2.0 kfp-1.8.11 kfp-pipeline-spec-0.1.17 kfp-server-api-1.8.5 kubernetes-18.20.0 pydantic-1.9.2 requests-toolbelt-0.10.1 strip-hints-0.1.10 tabulate-0.9.0 typer-0.7.0 typing-extensions-3.10.0.2 uritemplate-3.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install kfp==1.8.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9222df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.11\n"
     ]
    }
   ],
   "source": [
    "#!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70354cdf-1d82-4aa7-9a36-24a9384f3094",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "164ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import kfp\n",
    "import pprint\n",
    "import yaml\n",
    "from jinja2 import Template\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.compiler import compiler\n",
    "from kfp.v2.dsl import Dataset\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform, firestore\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe612cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id='petcircle-science-playground'\n",
    "project_number='734227425472'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27224ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_registry_location='australia-southeast1'\n",
    "af_registry_name='mlops-vertex-kit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe4d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_dir='../components/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82030efb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _load_custom_component(project_id: str,\n",
    "                           af_registry_location: str,\n",
    "                           af_registry_name: str,\n",
    "                           components_dir: str,\n",
    "                           component_name: str):\n",
    "    component_path = os.path.join(components_dir,\n",
    "                                component_name,\n",
    "                                'component.yaml.jinja')\n",
    "    with open(component_path, 'r') as f:\n",
    "        component_text = Template(f.read()).render(\n",
    "          project_id=project_id,\n",
    "          af_registry_location=af_registry_location,\n",
    "          af_registry_name=af_registry_name)\n",
    "\n",
    "    return kfp.components.load_component_from_text(component_text)\n",
    "\n",
    "load_custom_component = partial(_load_custom_component,\n",
    "                                project_id=project_id,\n",
    "                                af_registry_location=af_registry_location,\n",
    "                                af_registry_name=af_registry_name,\n",
    "                                components_dir=components_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495502ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_op = load_custom_component(component_name='data_preprocess')\n",
    "train_op = load_custom_component(component_name='train_model')\n",
    "check_metrics_op = load_custom_component(component_name='check_model_metrics')\n",
    "create_endpoint_op = load_custom_component(component_name='create_endpoint')\n",
    "test_endpoint_op = load_custom_component(component_name='test_endpoint')\n",
    "deploy_model_op = load_custom_component(component_name='deploy_model')\n",
    "monitor_model_op = load_custom_component(component_name='monitor_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026cb75",
   "metadata": {},
   "source": [
    "Then define the pipeline using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d36a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_region='australia-southeast1'\n",
    "pipeline_root='gs://vertex_pipeline_demo_root_hy_syd/pipeline_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954a7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_region='australia-southeast1'\n",
    "#input_dataset_uri='bq://petcircle-science-playground.vertex_pipeline_demo.banknote_authentication'\n",
    "input_dataset_uri='bq://petcircle-science-playground.datalake.review_product_2013_2022'\n",
    "gcs_data_output_folder='gs://vertex_pipeline_demo_root_hy_syd/datasets/training'\n",
    "training_data_schema='reviewtext:string;Class:int'\n",
    "\n",
    "data_pipeline_root='gs://vertex_pipeline_demo_root_hy_syd/compute_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6812d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_container_image_uri=f'{af_registry_location}-docker.pkg.dev/{project_id}/{af_registry_name}/training:latest'\n",
    "serving_container_image_uri=f'{af_registry_location}-docker.pkg.dev/{project_id}/{af_registry_name}/batch_prediction:latest'\n",
    "hpo_container_image_uri=f'{af_registry_location}-docker.pkg.dev/{project_id}/{af_registry_name}/hpo:latest'\n",
    "custom_job_service_account=f'{project_number}-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4574529c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('australia-southeast1-docker.pkg.dev/petcircle-science-playground/mlops-vertex-kit/training:latest',\n",
       " 'australia-southeast1-docker.pkg.dev/petcircle-science-playground/mlops-vertex-kit/batch_prediction:latest',\n",
       " '734227425472-compute@developer.gserviceaccount.com',\n",
       " 'australia-southeast1-docker.pkg.dev/petcircle-science-playground/mlops-vertex-kit/hpo:latest')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_container_image_uri,serving_container_image_uri,custom_job_service_account, hpo_container_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ab17d4-a9c4-4e59-8898-0198dc888e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "from google_cloud_pipeline_components.v1.hyperparameter_tuning_job import HyperparameterTuningJobRunOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from kfp.v2.components import importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d425713-cdd7-48fd-81d1-03f9e1d75caf",
   "metadata": {},
   "source": [
    "## HPO pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "29213faa-3bec-444a-891f-4610a7b25398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import Dataset, Input, Metrics, Model, Output\n",
    "\n",
    "@component\n",
    "def worker_pool_specs(project_id: str,\n",
    "    data_region: str,\n",
    "    data_pipeline_root: str,\n",
    "    hpo_container_image_uri: str,\n",
    "    custom_job_service_account: str,\n",
    "    warehouse: str,\n",
    "    input_dataset: Input[Dataset]\n",
    "                     ) -> list:\n",
    "    \"\"\"\n",
    "    Pass the preprocessed data uri to HPO as a worker pool argument. The vanilla HPO API \n",
    "    doesn't support 'input data' so it's done this way.\n",
    "    \n",
    "    data_preprocess -> dataset.uri -> CMDARGS -> worker_pool_specs -> HPO\n",
    "    \"\"\"\n",
    "    \n",
    "    task_type = 'training'\n",
    "    display_name = 'hpo-pipeline-template'\n",
    "    CMDARGS = [\n",
    "    \"--training_data_uri=\"+str(input_dataset.uri),\n",
    "    \"--warehouse=\"+warehouse,\n",
    "    #\"--training_data_uri=gs://vertex_pipeline_demo_root_hy_syd/datasets/training/processed_data-20230118012508.csv\"\n",
    "    ]\n",
    "\n",
    "    # The spec of the worker pools including machine type and Docker image\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\"image_uri\": hpo_container_image_uri, \"args\": CMDARGS},\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    return worker_pool_specs\n",
    "\n",
    "@component(packages_to_install=['google-cloud-firestore==2.3'])\n",
    "def best_hpo_to_args(hpo_best: str,\n",
    "                    project_id: str,\n",
    "                    solution_name: str,\n",
    "                    as_at_date: str,\n",
    "                    warehouse: str) -> str:\n",
    "    \"\"\"\n",
    "    Write the best HPO params to firestore. \n",
    "    We keep the output to chain this component to the hpo_completion step.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "    from google.cloud import firestore\n",
    "    hpo_best = json.loads(hpo_best.replace(\"'\", '\"'))\n",
    "\n",
    "    hpo_best_dict = {}\n",
    "    \n",
    "    for i in hpo_best['parameters']:\n",
    "        hpo_best_dict.update({i['parameterId']: i['value']})\n",
    "    \n",
    "    for i in hpo_best['finalMeasurement']['metrics']:\n",
    "        hpo_best_dict.update({i['metricId']: i['value']})\n",
    "    \n",
    "    db = firestore.Client(project=project_id)\n",
    "    db.collection(\"models\").document(solution_name).collection(\"HPO\").document(\n",
    "        as_at_date).collection(warehouse).document(\"params\").set(hpo_best_dict,merge=True)\n",
    "    \n",
    "    hpo_best_dict.update({'warehouse': warehouse})\n",
    "    hpo_best_dict=str(hpo_best_dict).replace(\"'\", '\"')\n",
    "    \n",
    "    return hpo_best_dict\n",
    "\n",
    "@component\n",
    "def hpo_completion(hpo_flags: list) -> str:\n",
    "    \"\"\"\n",
    "    This function doesn nothing but wait to merge all the async HPO jobs so as \n",
    "    to gurantee that the following training module gets the latest params from\n",
    "    firestore for all warehouses.\n",
    "    \"\"\"\n",
    "    return \"true\"\n",
    "\n",
    "def hpo_warehouse(project_id,\n",
    "                 data_region,\n",
    "                 data_pipeline_root,\n",
    "                 preprocess_task,\n",
    "                 display_name,\n",
    "                 metric_spec,\n",
    "                 parameter_spec,\n",
    "                 warehouse\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    This is not a component function. It's a normal function that generates the sub graph \n",
    "    for every warehouse. We return the pipeline operation to chain this component to the \n",
    "    hpo_completion step. \n",
    "    \"\"\"\n",
    "    \n",
    "    worker_pool_specs_op = worker_pool_specs(project_id=project_id,\n",
    "    data_region=data_region,\n",
    "    data_pipeline_root=data_pipeline_root,\n",
    "    hpo_container_image_uri=hpo_container_image_uri,\n",
    "    custom_job_service_account=custom_job_service_account,\n",
    "    warehouse=warehouse,                               \n",
    "    input_dataset=preprocess_task.outputs['output_dataset']\n",
    "    )\n",
    "\n",
    "    tuning_op = HyperparameterTuningJobRunOp(\n",
    "    display_name=display_name+'-'+warehouse,\n",
    "    project=project_id,\n",
    "    location=data_region,\n",
    "    worker_pool_specs=worker_pool_specs_op.output,\n",
    "    study_spec_metrics=metric_spec,\n",
    "    study_spec_parameters=parameter_spec,\n",
    "    max_trial_count=4,\n",
    "    parallel_trial_count=2,\n",
    "    base_output_directory=data_pipeline_root,\n",
    "    study_spec_algorithm='GRID_SEARCH'\n",
    "    )\n",
    " \n",
    "    trials_op = hyperparameter_tuning_job.GetTrialsOp(\n",
    "        gcp_resources=tuning_op.outputs[\"gcp_resources\"]\n",
    "    )\n",
    "\n",
    "    best_trial_op = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "        trials=trials_op.output, study_spec_metrics=metric_spec\n",
    "    )\n",
    "    \n",
    "    best_hpo_to_args_op = best_hpo_to_args(best_trial_op.output,\n",
    "                                          project_id=project_id,               \n",
    "                                        as_at_date=datetime.now().strftime('%Y-%m-%d'),\n",
    "                                        warehouse=warehouse,\n",
    "                                          solution_name=display_name)\n",
    "    return best_hpo_to_args_op\n",
    "    \n",
    "\n",
    "@dsl.pipeline(name='hpo-pipeline-template')\n",
    "def pipeline(project_id: str,\n",
    "             data_region: str,\n",
    "             gcs_data_output_folder: str,\n",
    "             input_dataset_uri: str,\n",
    "             training_data_schema: str,\n",
    "             data_pipeline_root: str,\n",
    "             \n",
    "             training_container_image_uri: str,\n",
    "             serving_container_image_uri: str,\n",
    "             custom_job_service_account: str,\n",
    "             hptune_region: str,\n",
    "             hp_config_suggestions_per_request: int,\n",
    "             hp_config_max_trials: int,\n",
    "             \n",
    "             metrics_name: str,\n",
    "             metrics_threshold: float,\n",
    "             \n",
    "             endpoint_machine_type: str,\n",
    "             endpoint_min_replica_count: int,\n",
    "             endpoint_max_replica_count: int,\n",
    "             endpoint_test_instances: str,\n",
    "             \n",
    "             output_model_file_name: str = 'model.h5',\n",
    "             machine_type: str = \"n1-standard-8\",\n",
    "             accelerator_count: int = 0,\n",
    "             accelerator_type: str = 'ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "             vpc_network: str = \"\",\n",
    "             enable_model_monitoring: str = 'False',\n",
    "            task_type: str = 'training'):\n",
    "    \n",
    "    task_type = 'training'\n",
    "    display_name = 'hpo-pipeline-template'\n",
    "    metric_spec = hyperparameter_tuning_job.serialize_metrics({\"val_balanced_acc\": \"maximize\"})\n",
    "    parameter_spec = hyperparameter_tuning_job.serialize_parameters(\n",
    "    {\n",
    "        \"batch_size\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[32, 64], scale=None\n",
    "        ),\n",
    "        \"lr\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[0.0001, 0.0002], scale=None\n",
    "        ),\n",
    "    }\n",
    "    )\n",
    "\n",
    "    preprocess_task = preprocess_op(\n",
    "      project_id=project_id,\n",
    "      data_region=data_region,\n",
    "      gcs_output_folder=gcs_data_output_folder,\n",
    "      gcs_output_format=\"CSV\",\n",
    "      task_type=task_type)\n",
    "\n",
    "    hpo_op_ec = hpo_warehouse(project_id,\n",
    "             data_region,\n",
    "             data_pipeline_root,\n",
    "             preprocess_task,\n",
    "             display_name,\n",
    "             metric_spec,\n",
    "             parameter_spec,\n",
    "            \"EC\")\n",
    "    \n",
    "    hpo_op_mel = hpo_warehouse(project_id,\n",
    "                 data_region,\n",
    "                 data_pipeline_root,\n",
    "                 preprocess_task,\n",
    "                 display_name,\n",
    "                 metric_spec,\n",
    "                 parameter_spec,\n",
    "                \"MEL\")\n",
    "    \n",
    "    hpo_completion_op = hpo_completion([str(hpo_op_ec.output), \n",
    "                                        str(hpo_op_mel.output)])\n",
    "    \n",
    "    with dsl.Condition(\n",
    "         hpo_completion_op.output==\"true\",\n",
    "        name=\"train_model\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    We use the condition module to check if all HPO jobs for different warehouse are finished so as to\n",
    "    kick off the training step at the right time.\n",
    "    \"\"\"\n",
    "        train_task = train_op(\n",
    "          project_id=project_id,\n",
    "          data_region=data_region,\n",
    "          data_pipeline_root=data_pipeline_root,\n",
    "          input_data_schema=training_data_schema,\n",
    "          training_container_image_uri=training_container_image_uri,\n",
    "          serving_container_image_uri=serving_container_image_uri,\n",
    "          custom_job_service_account=custom_job_service_account,\n",
    "          input_dataset=preprocess_task.outputs['output_dataset'],\n",
    "          output_model_file_name=output_model_file_name,\n",
    "          machine_type=machine_type,\n",
    "          accelerator_count=accelerator_count,\n",
    "          accelerator_type=accelerator_type,\n",
    "          hptune_region=hptune_region,\n",
    "          hp_config_max_trials=hp_config_max_trials,\n",
    "          hp_config_suggestions_per_request=hp_config_suggestions_per_request,\n",
    "          vpc_network=vpc_network\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fba56",
   "metadata": {},
   "source": [
    "### Compile and run the end-to-end HPO pipeline\n",
    "With our full pipeline defined, it's time to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "794c18bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/australia-southeast1/pipelines/runs/hpo-pipeline-template-20230118224039?project=petcircle-science-playground\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=\"training_pipeline_job.json\"\n",
    ")\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=project_id,\n",
    "    region=pipeline_region)\n",
    "\n",
    "\n",
    "test_instances = json.dumps([\n",
    "\t\t{'reviewtext': 'pet circle is not recommended',\"Class\":\"0\"},\n",
    "\t\t{'reviewtext': 'pet circle is highly recommended',\"Class\":\"1\"},\n",
    "\t\t{'reviewtext': 'think twice before you buy',\"Class\":\"0\"},\n",
    "\t\t{'reviewtext': 'great product. will buy again.',\"Class\":\"1\"}\n",
    "\t\t])\n",
    "\n",
    "pipeline_params = {\n",
    "    'project_id': project_id,\n",
    "    'data_region': data_region,\n",
    "    'gcs_data_output_folder': gcs_data_output_folder,\n",
    "    'output_model_file_name': 'model.h5',\n",
    "    'input_dataset_uri': input_dataset_uri,\n",
    "    'training_data_schema': training_data_schema,\n",
    "    'data_pipeline_root': data_pipeline_root,\n",
    "    \n",
    "    'training_container_image_uri': training_container_image_uri,\n",
    "    'serving_container_image_uri': serving_container_image_uri,\n",
    "    'custom_job_service_account': custom_job_service_account,\n",
    "    'hptune_region':\"asia-east1\",\n",
    "    'hp_config_suggestions_per_request': 5,\n",
    "    'hp_config_max_trials': 30,\n",
    "    \n",
    "    'metrics_name': 'au_prc',\n",
    "    'metrics_threshold': 0.3,\n",
    "    \n",
    "    'endpoint_machine_type': 'n1-standard-4',\n",
    "    'endpoint_min_replica_count': 1,\n",
    "    'endpoint_max_replica_count': 1,\n",
    "    'endpoint_test_instances': test_instances\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"training_pipeline_job.json\", \n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=pipeline_params,\n",
    "    enable_caching=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233beb69-795d-43f7-87d7-57eb2afca324",
   "metadata": {},
   "source": [
    "## Trouble shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1185f6b2-d4c8-4c1a-abf0-478e564d2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import Dataset, Input, Metrics, Model, Output\n",
    "\n",
    "@component\n",
    "def worker_pool_specs(project_id: str,\n",
    "    data_region: str,\n",
    "    data_pipeline_root: str,\n",
    "    hpo_container_image_uri: str,\n",
    "    custom_job_service_account: str,\n",
    "    warehouse: str,\n",
    "   # input_dataset: Input[Dataset]\n",
    "                     ) -> list:\n",
    "\n",
    "    task_type = 'training'\n",
    "    display_name = 'hpo-pipeline-template'\n",
    "    CMDARGS = [\n",
    "    #\"--training_data_uri=\"+str(input_dataset.uri),\n",
    "    \"--warehouse=\"+warehouse,\n",
    "    \"--training_data_uri=gs://vertex_pipeline_demo_root_hy_syd/datasets/training/processed_data-20230118012508.csv\"\n",
    "    ]\n",
    "\n",
    "    # The spec of the worker pools including machine type and Docker image\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\"image_uri\": hpo_container_image_uri, \"args\": CMDARGS},\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    return worker_pool_specs\n",
    "\n",
    "@component(packages_to_install=['google-cloud-firestore==2.3'])\n",
    "def best_hpo_to_args(hpo_best: str,\n",
    "                    project_id: str,\n",
    "                    solution_name: str,\n",
    "                    as_at_date: str,\n",
    "                    warehouse: str) -> str:\n",
    "    import json\n",
    "    from google.cloud import firestore\n",
    "    hpo_best = json.loads(hpo_best.replace(\"'\", '\"'))\n",
    "\n",
    "    hpo_best_dict = {}\n",
    "    \n",
    "    for i in hpo_best['parameters']:\n",
    "        hpo_best_dict.update({i['parameterId']: i['value']})\n",
    "    \n",
    "    for i in hpo_best['finalMeasurement']['metrics']:\n",
    "        hpo_best_dict.update({i['metricId']: i['value']})\n",
    "    \n",
    "    db = firestore.Client(project=project_id)\n",
    "    db.collection(\"models\").document(solution_name).collection(\"HPO\").document(\n",
    "        as_at_date).collection(warehouse).document(\"params\").set(hpo_best_dict,merge=True)\n",
    "    \n",
    "    hpo_best_dict.update({'warehouse': warehouse})\n",
    "    hpo_best_dict=str(hpo_best_dict).replace(\"'\", '\"')\n",
    "    \n",
    "    return hpo_best_dict\n",
    "\n",
    "#@component\n",
    "# def hpo_completion(hpo_flags_1: str, hpo_flags_2: str) -> str:\n",
    "#     if hpo_flags_1 and hpo_flags_2:\n",
    "#         return \"true\"\n",
    "\n",
    "@component\n",
    "def hpo_completion(hpo_flags: list) -> str:\n",
    "    return \"true\"\n",
    "\n",
    "def hpo_warehouse(project_id,\n",
    "                 data_region,\n",
    "                 data_pipeline_root,\n",
    "                 #preprocess_task,\n",
    "                 display_name,\n",
    "                 metric_spec,\n",
    "                 parameter_spec,\n",
    "                 warehouse,\n",
    "                 gcp_resources\n",
    "                 ):\n",
    "#     worker_pool_specs_op = worker_pool_specs(project_id=project_id,\n",
    "#     data_region=data_region,\n",
    "#     data_pipeline_root=data_pipeline_root,\n",
    "#     hpo_container_image_uri=hpo_container_image_uri,\n",
    "#     custom_job_service_account=custom_job_service_account,\n",
    "#     warehouse=warehouse,\n",
    "# #    input_dataset=\"gs://vertex_pipeline_demo_root_hy_syd/datasets/training/processed_data-20230118012508.csv\"                                 \n",
    "#     #input_dataset=preprocess_task.outputs['output_dataset'])\n",
    "#     )\n",
    "\n",
    "#     tuning_op = HyperparameterTuningJobRunOp(\n",
    "#     display_name=display_name+'-'+warehouse,\n",
    "#     project=project_id,\n",
    "#     location=data_region,\n",
    "#     worker_pool_specs=worker_pool_specs_op.output,\n",
    "#     study_spec_metrics=metric_spec,\n",
    "#     study_spec_parameters=parameter_spec,\n",
    "#     max_trial_count=4,\n",
    "#     parallel_trial_count=2,\n",
    "#     base_output_directory=data_pipeline_root,\n",
    "#     study_spec_algorithm='GRID_SEARCH'\n",
    "#     )\n",
    " \n",
    "    trials_op = hyperparameter_tuning_job.GetTrialsOp(\n",
    "#        gcp_resources=tuning_op.outputs[\"gcp_resources\"]\n",
    "        gcp_resources=gcp_resources\n",
    "#        gcp_resources='{\"resources\":[{\"resourceType\":\"HyperparameterTuningJob\",\"resourceUri\":\"https://australia-southeast1-aiplatform.googleapis.com/v1/projects/734227425472/locations/australia-southeast1/hyperparameterTuningJobs/695071668661387264\"}]}'\n",
    "\n",
    "    )\n",
    "\n",
    "    best_trial_op = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "        trials=trials_op.output, study_spec_metrics=metric_spec\n",
    "    )\n",
    "    \n",
    "    best_hpo_to_args_op = best_hpo_to_args(best_trial_op.output,\n",
    "                                          project_id=project_id,               \n",
    "                                        as_at_date=datetime.now().strftime('%Y-%m-%d'),\n",
    "                                        warehouse=warehouse,\n",
    "                                          solution_name=display_name)\n",
    "    return best_hpo_to_args_op\n",
    "\n",
    "    \n",
    "@dsl.pipeline(name='hpo-pipeline-template')\n",
    "def pipeline(project_id: str,\n",
    "             data_region: str,\n",
    "             gcs_data_output_folder: str,\n",
    "             input_dataset_uri: str,\n",
    "             training_data_schema: str,\n",
    "             data_pipeline_root: str,\n",
    "             \n",
    "             training_container_image_uri: str,\n",
    "             serving_container_image_uri: str,\n",
    "             custom_job_service_account: str,\n",
    "             hptune_region: str,\n",
    "             hp_config_suggestions_per_request: int,\n",
    "             hp_config_max_trials: int,\n",
    "             \n",
    "             metrics_name: str,\n",
    "             metrics_threshold: float,\n",
    "             \n",
    "             endpoint_machine_type: str,\n",
    "             endpoint_min_replica_count: int,\n",
    "             endpoint_max_replica_count: int,\n",
    "             endpoint_test_instances: str,\n",
    "             \n",
    "             output_model_file_name: str = 'model.h5',\n",
    "             machine_type: str = \"n1-standard-8\",\n",
    "             accelerator_count: int = 0,\n",
    "             accelerator_type: str = 'ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "             vpc_network: str = \"\",\n",
    "             enable_model_monitoring: str = 'False',\n",
    "            task_type: str = 'training'):\n",
    "    \n",
    "    task_type = 'training'\n",
    "    display_name = 'hpo-pipeline-template'\n",
    "    metric_spec = hyperparameter_tuning_job.serialize_metrics({\"val_balanced_acc\": \"maximize\"})\n",
    "    parameter_spec = hyperparameter_tuning_job.serialize_parameters(\n",
    "    {\n",
    "        \"batch_size\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[32, 64], scale=None\n",
    "        ),\n",
    "        \"lr\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[0.0001, 0.0002], scale=None\n",
    "        ),\n",
    "    }\n",
    "    )\n",
    "\n",
    "    preprocess_task = preprocess_op(\n",
    "      project_id=project_id,\n",
    "      data_region=data_region,\n",
    "      gcs_output_folder=gcs_data_output_folder,\n",
    "      gcs_output_format=\"CSV\",\n",
    "      task_type=task_type)\n",
    "\n",
    "#     worker_pool_specs_op = worker_pool_specs(project_id=project_id,\n",
    "#     data_region=data_region,\n",
    "#     data_pipeline_root=data_pipeline_root,\n",
    "#     hpo_container_image_uri=hpo_container_image_uri,\n",
    "#     custom_job_service_account=custom_job_service_account,\n",
    "#     input_dataset=preprocess_task.outputs['output_dataset'])\n",
    "\n",
    "#     tuning_op = HyperparameterTuningJobRunOp(\n",
    "#     display_name=display_name,\n",
    "#     project=project_id,\n",
    "#     location=data_region,\n",
    "#     worker_pool_specs=worker_pool_specs_op.output,\n",
    "#     study_spec_metrics=metric_spec,\n",
    "#     study_spec_parameters=parameter_spec,\n",
    "#     max_trial_count=4,\n",
    "#     parallel_trial_count=2,\n",
    "#     base_output_directory=data_pipeline_root,\n",
    "#     study_spec_algorithm='GRID_SEARCH'\n",
    "#     )\n",
    "    \n",
    "#     trials_op = hyperparameter_tuning_job.GetTrialsOp(\n",
    "#         gcp_resources=tuning_op.outputs[\"gcp_resources\"]\n",
    "# #        gcp_resources='{\"resources\":[{\"resourceType\":\"HyperparameterTuningJob\",\"resourceUri\":\"https://australia-southeast1-aiplatform.googleapis.com/v1/projects/734227425472/locations/australia-southeast1/hyperparameterTuningJobs/695071668661387264\"}]}'\n",
    "\n",
    "#     )\n",
    "\n",
    "#     best_trial_op = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "#         trials=trials_op.output, study_spec_metrics=metric_spec\n",
    "#     )\n",
    "\n",
    "    hpo_op_ec = hpo_warehouse(project_id,\n",
    "             data_region,\n",
    "             data_pipeline_root,\n",
    "             #preprocess_task,\n",
    "             display_name,\n",
    "             metric_spec,\n",
    "             parameter_spec,\n",
    "            \"EC\",\n",
    "            '{\"resources\":[{\"resourceType\":\"HyperparameterTuningJob\",\"resourceUri\":\"https://australia-southeast1-aiplatform.googleapis.com/v1/projects/734227425472/locations/australia-southeast1/hyperparameterTuningJobs/4881167522302263296\"}]}'\n",
    "            )\n",
    "    \n",
    "    hpo_op_mel = hpo_warehouse(project_id,\n",
    "                 data_region,\n",
    "                 data_pipeline_root,\n",
    "                 #preprocess_task,\n",
    "                 display_name,\n",
    "                 metric_spec,\n",
    "                 parameter_spec,\n",
    "                \"MEL\",\n",
    "                '{\"resources\":[{\"resourceType\":\"HyperparameterTuningJob\",\"resourceUri\":\"https://australia-southeast1-aiplatform.googleapis.com/v1/projects/734227425472/locations/australia-southeast1/hyperparameterTuningJobs/2070921354823073792\"}]}'     \n",
    "                              )\n",
    "    \n",
    "    hpo_completion_op = hpo_completion([str(hpo_op_ec.output), \n",
    "                                        str(hpo_op_mel.output)])\n",
    "    \n",
    "    with dsl.Condition(\n",
    "         hpo_completion_op.output==\"true\",\n",
    "        name=\"train_model\"\n",
    "    ):\n",
    "    \n",
    "        train_task = train_op(\n",
    "          project_id=project_id,\n",
    "          data_region=data_region,\n",
    "          data_pipeline_root=data_pipeline_root,\n",
    "          input_data_schema=training_data_schema,\n",
    "          training_container_image_uri=training_container_image_uri,\n",
    "          serving_container_image_uri=serving_container_image_uri,\n",
    "          custom_job_service_account=custom_job_service_account,\n",
    "          input_dataset=preprocess_task.outputs['output_dataset'],\n",
    "          output_model_file_name=output_model_file_name,\n",
    "          machine_type=machine_type,\n",
    "          accelerator_count=accelerator_count,\n",
    "          accelerator_type=accelerator_type,\n",
    "          hptune_region=hptune_region,\n",
    "          hp_config_max_trials=hp_config_max_trials,\n",
    "          hp_config_suggestions_per_request=hp_config_suggestions_per_request,\n",
    "          vpc_network=vpc_network\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
